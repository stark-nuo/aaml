{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "batch_size = 1\n",
    "epochs = 5000\n",
    "model = SimpleNet(1, 1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x = np.array([1, 2, 3, 4, 1.1, 2.2, 3.3])\n",
    "y_true = x**2\n",
    "inputs = torch.FloatTensor(x).view(-1, 1)\n",
    "labels = torch.FloatTensor(y_true).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(optimizer, model, inputs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    globals()['w'] = model.fc1.weight.item()\n",
    "    globals()['b'] = model.fc1.bias.item()\n",
    "    print(\"model.weight:{}, model.bias:{}\".format(w, b))\n",
    "    loss = criterion(outputs, labels)\n",
    "    show = torch.cat((inputs, outputs, labels), dim=1)\n",
    "    print(\"input, output, label\\n\", show.T)\n",
    "    print(\"loss:{}\".format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.weight:0.44418656826019287, model.bias:0.5152027606964111\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 0.9594,  1.4036,  1.8478,  2.2919,  1.0038,  1.4924,  1.9810],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:48.06100845336914\n"
     ]
    }
   ],
   "source": [
    "# epoch 0\n",
    "pre_output = train_one_epoch(optimizer, model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True]])\n",
      "theroy_loss:48.06100845336914\n"
     ]
    }
   ],
   "source": [
    "# calculate theory\n",
    "theory_output = inputs*w + b\n",
    "print(theory_output.T == pre_output.T)\n",
    "print(\"theroy_loss:{}\".format((labels-pre_output).pow(2).sum()/len(theory_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward\n",
    "$$\n",
    "MSE\\_Loss=\\sum_{n=1}^{N}(y_n-c_n)^2 \\\\\n",
    "y=w*x+b \\\\\n",
    "$$ \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}=2*(y_n-c_n)*x=\\Delta{_w} \\\\\n",
    "w_{n+1}=w_n-lr*\\Delta{_w}=w_n-2*lr*(y_n-c_n)*x \\\\\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}=2*(y_n-c_n)=\\Delta{_b} \\\\\n",
    "b_{n+1}=b_n-2*lr*(y_n-c_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6878344872548996\n",
      "0.617945852279663\n"
     ]
    }
   ],
   "source": [
    "# Therefore, the next epoch weight should be:\n",
    "dif = pre_output - labels\n",
    "print(w - 2*0.01*float(sum(dif)/len(dif))*float(sum(inputs)/len(inputs)))\n",
    "# bias should be:\n",
    "print(b - 2*0.01*float(sum(dif)/len(dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.weight:0.7827966213226318, model.bias:0.6179458498954773\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 1.4007,  2.1835,  2.9663,  3.7491,  1.4790,  2.3401,  3.2012],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:36.484130859375\n"
     ]
    }
   ],
   "source": [
    "# epoch 1\n",
    "pre_output = train_one_epoch(optimizer, model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next weight 0.9834869326622424\n",
      "next bias 0.7025742888450622\n",
      "theroy_loss:36.484130859375\n",
      "model.weight:1.0711212158203125, model.bias:0.7025743126869202\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 1.7737,  2.8448,  3.9159,  4.9871,  1.8808,  3.0590,  4.2373],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:28.13516616821289\n"
     ]
    }
   ],
   "source": [
    "# epoch 2\n",
    "dif = pre_output - labels\n",
    "print(\"next weight\", w - 2*0.01*float(sum(dif)/len(dif))*float(sum(inputs)/len(inputs)))\n",
    "print(\"next bias\", b - 2*0.01*float(sum(dif)/len(dif)))\n",
    "print(\"theroy_loss:{}\".format((labels-pre_output).pow(2).sum()/len(theory_output)))\n",
    "pre_output = train_one_epoch(optimizer, model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 要弄清楚SGD怎么更新的，按道理来说十分接近了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next weight 1.2353688600608563\n",
      "next bias 0.7718353629112243\n",
      "theroy_loss:28.13516616821289\n",
      "model.weight:1.3167636394500732, model.bias:0.7718353867530823\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 2.0886,  3.4054,  4.7221,  6.0389,  2.2203,  3.6687,  5.1172],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:22.11155128479004\n",
      "next weight 1.450098077727613\n",
      "next bias 0.8280607485771179\n",
      "theroy_loss:22.11155128479004\n",
      "model.weight:1.526176929473877, model.bias:0.8280607461929321\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 2.3542,  3.8804,  5.4066,  6.9328,  2.5069,  4.1856,  5.8644],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:17.76310920715332\n",
      "next weight 1.6332912488588591\n",
      "next bias 0.8732294321060181\n",
      "theroy_loss:17.76310920715332\n",
      "model.weight:1.7048382759094238, model.bias:0.873229444026947\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 2.5781,  4.2829,  5.9877,  7.6926,  2.7486,  4.6239,  6.4992],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:14.621459007263184\n",
      "next weight 1.7897156281689781\n",
      "next bias 0.9090210962295532\n",
      "theroy_loss:14.621459007263184\n",
      "model.weight:1.8573962450027466, model.bias:0.9090210795402527\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 2.7664,  4.6238,  6.4812,  8.3386,  2.9522,  4.9953,  7.0384],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:12.349210739135742\n",
      "next weight 1.9234173334007603\n",
      "next bias 0.9368612957000733\n",
      "theroy_loss:12.349210739135742\n",
      "model.weight:1.987796425819397, model.bias:0.9368612766265869\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 2.9247,  4.9125,  6.9003,  8.8880,  3.1234,  5.3100,  7.4966],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:10.703299522399902\n",
      "next weight 2.0378305220281767\n",
      "next bias 0.9579599905014038\n",
      "theroy_loss:10.703299522399902\n",
      "model.weight:2.0993876457214355, model.bias:0.9579600095748901\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.0573,  5.1567,  7.2561,  9.3555,  3.2673,  5.5766,  7.8859],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:9.508638381958008\n",
      "next weight 2.1358700118666714\n",
      "next bias 0.9733441388607025\n",
      "theroy_loss:9.508638381958008\n",
      "model.weight:2.195012331008911, model.bias:0.9733441472053528\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.1684,  5.3634,  7.5584,  9.7534,  3.3879,  5.8024,  8.2169],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:8.6390962600708\n",
      "next weight 2.2200098080461053\n",
      "next bias 0.9838852512836457\n",
      "theroy_loss:8.6390962600708\n",
      "model.weight:2.277082681655884, model.bias:0.9838852286338806\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.2610,  5.5381,  7.8151, 10.0922,  3.4887,  5.9935,  8.4983],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:8.003812789916992\n",
      "next weight 2.292349475251399\n",
      "next bias 0.9903230327367782\n",
      "theroy_loss:8.003812789916992\n",
      "model.weight:2.3476462364196777, model.bias:0.990323007106781\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.3380,  5.6856,  8.0333, 10.3809,  3.5727,  6.1551,  8.7376],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:7.537330627441406\n",
      "next weight 2.3546711689362074\n",
      "next bias 0.9932853278517723\n",
      "theroy_loss:7.537330627441406\n",
      "model.weight:2.4084408283233643, model.bias:0.9932853579521179\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.4017,  5.8102,  8.2186, 10.6270,  3.6426,  6.2919,  8.9411],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:7.192501544952393\n",
      "next weight 2.4084874780662577\n",
      "next bias 0.9933050295291469\n",
      "theroy_loss:7.192501544952393\n",
      "model.weight:2.4609415531158447, model.bias:0.9933050274848938\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.4542,  5.9152,  8.3761, 10.8371,  3.7003,  6.4074,  9.1144],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.935356140136719\n",
      "next weight 2.4550823242326976\n",
      "next bias 0.9908342684805393\n",
      "theroy_loss:6.935356140136719\n",
      "model.weight:2.5064003467559814, model.bias:0.9908342957496643\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.4972,  6.0036,  8.5100, 11.0164,  3.7479,  6.5049,  9.2620],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.741427898406982\n",
      "next weight 2.4955453990763945\n",
      "next bias 0.9862569084763527\n",
      "theroy_loss:6.741427898406982\n",
      "model.weight:2.545879602432251, model.bias:0.9862568974494934\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.5321,  6.0780,  8.6239, 11.1698,  3.7867,  6.5872,  9.3877],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.593087196350098\n",
      "next weight 2.530801387080189\n",
      "next bias 0.9798986142873765\n",
      "theroy_loss:6.593087196350098\n",
      "model.weight:2.5802812576293945, model.bias:0.9798986315727234\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.5602,  6.1405,  8.7207, 11.3010,  3.8182,  6.6565,  9.4948],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.477636814117432\n",
      "next weight 2.5616353382155355\n",
      "next bias 0.9720358949899673\n",
      "theroy_loss:6.477636814117432\n",
      "model.weight:2.610370635986328, model.bias:0.9720358848571777\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.5824,  6.1928,  8.8031, 11.4135,  3.8434,  6.7149,  9.5863],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.385927677154541\n",
      "next weight 2.5887133702036356\n",
      "next bias 0.9629033035039902\n",
      "theroy_loss:6.385927677154541\n",
      "model.weight:2.6367974281311035, model.bias:0.9629033207893372\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.5997,  6.2365,  8.8733, 11.5101,  3.8634,  6.7639,  9.6643],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.311371803283691\n",
      "next weight 2.6126009994256414\n",
      "next bias 0.9527000081539154\n",
      "theroy_loss:6.311371803283691\n",
      "model.weight:2.6601133346557617, model.bias:0.9527000188827515\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.6128,  6.2729,  8.9330, 11.5932,  3.8788,  6.8049,  9.7311],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.2492194175720215\n",
      "next weight 2.6337784127820085\n",
      "next bias 0.9415949320793152\n",
      "theroy_loss:6.2492194175720215\n",
      "model.weight:2.6807861328125, model.bias:0.9415949583053589\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.6224,  6.3032,  8.9840, 11.6647,  3.8905,  6.8393,  9.7882],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.19605016708374\n",
      "next weight 2.6526527618826474\n",
      "next bias 0.9297314894199371\n",
      "theroy_loss:6.19605016708374\n",
      "model.weight:2.6992130279541016, model.bias:0.9297314882278442\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.6289,  6.3282,  9.0274, 11.7266,  3.8989,  6.8680,  9.8371],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.149402141571045\n",
      "next weight 2.669569792866682\n",
      "next bias 0.9172313296794892\n",
      "theroy_loss:6.149402141571045\n",
      "model.weight:2.715731382369995, model.bias:0.9172313213348389\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.6330,  6.3487,  9.0644, 11.7802,  3.9045,  6.8918,  9.8791],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.1074981689453125\n"
     ]
    }
   ],
   "source": [
    "# epoch test\n",
    "ws, bs = [], []\n",
    "for _ in range(20):\n",
    "    dif = pre_output - labels\n",
    "    print(\"next weight\", w - 2*0.01*float(sum(dif)/len(dif))*float(sum(inputs)/len(inputs)))\n",
    "    print(\"next bias\", b - 2*0.01*float(sum(dif)/len(dif)))\n",
    "    print(\"theroy_loss:{}\".format((labels-pre_output).pow(2).sum()/len(theory_output)))\n",
    "    pre_output = train_one_epoch(optimizer, model, inputs)\n",
    "    ws.append(w)\n",
    "    bs.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0883068  0.07533912 0.06433167 0.05498802 0.04705654 0.04032366\n",
      " 0.03460798 0.02975571 0.02563627 0.02213886 0.01916937 0.01664788\n",
      " 0.01450672 0.01268829 0.01114383 0.00983201 0.00871744 0.00777038\n",
      " 0.00696557]\n",
      "[ 5.62253594e-02  4.51686978e-02  3.57916355e-02  2.78401971e-02\n",
      "  2.10987329e-02  1.53841376e-02  1.05410814e-02  6.43777847e-03\n",
      "  2.96235085e-03  1.96695328e-05 -2.47073174e-03 -4.57739830e-03\n",
      " -6.35826588e-03 -7.86274672e-03 -9.13256407e-03 -1.02033019e-02\n",
      " -1.11050606e-02 -1.18634701e-02 -1.25001669e-02]\n"
     ]
    }
   ],
   "source": [
    "# 按道理它们应该是一样的\n",
    "print(np.diff(ws)/float(sum(inputs)/len(inputs)))\n",
    "print(np.diff(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果不一样。既然bias比较接近，那用bias去预估weight，看下会不会贴近\n",
    "pb = b\n",
    "pw = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.weight:2.7306270599365234, model.bias:0.9041976928710938\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.6348,  6.3655,  9.0961, 11.8267,  3.9079,  6.9116,  9.9153],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.069058895111084\n",
      "dw -0.00628130940221845\n",
      "db 0.013033628463745117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# backward\n",
    "pre_output = train_one_epoch(optimizer, model, inputs)\n",
    "# get diff\n",
    "db = pb - b\n",
    "dw = pw - w\n",
    "print(\"dw\", dw/float(sum(inputs)/len(inputs)))\n",
    "print(\"db\", db)\n",
    "pb = b\n",
    "pw = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果发现，实际代码与理论公式有哪里出入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.weight:2.74414324760437, model.bias:0.8907182812690735\n",
      "input, output, label\n",
      " tensor([[ 1.0000,  2.0000,  3.0000,  4.0000,  1.1000,  2.2000,  3.3000],\n",
      "        [ 3.6349,  6.3790,  9.1231, 11.8673,  3.9093,  6.9278,  9.9464],\n",
      "        [ 1.0000,  4.0000,  9.0000, 16.0000,  1.2100,  4.8400, 10.8900]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss:6.033162593841553\n",
      "weight.grad:-1.2342838048934937, bias.grad:1.385087251663208\n",
      "dw -0.01351618766784668\n",
      "db 0.013479411602020264\n"
     ]
    }
   ],
   "source": [
    "# 看一下网络计算的梯度\n",
    "pre_output = train_one_epoch(optimizer, model, inputs)\n",
    "print(\"weight.grad:{}, bias.grad:{}\".format(model.fc1.weight.grad.item(), model.fc1.bias.grad.item()))\n",
    "db = pb - b\n",
    "dw = pw - w\n",
    "print(\"dw\", dw)\n",
    "print(\"db\", db)\n",
    "pb = b\n",
    "pw = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "差值并不是单纯乘上LR，说明SGD还是在作用的\n",
    "接下来就验证梯度的计算就好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2343], grad_fn=<DivBackward0>)\n",
      "tensor([1.3851], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sum(2*(pre_output-labels)*inputs)/len(inputs))\n",
    "print(sum(2*(pre_output-labels))/len(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
