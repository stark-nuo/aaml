{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell命令模式目前支持的Jupyter Notebook快捷\n",
    "- Enter : 转入编辑模式\n",
    "- Shift-Enter : 运行本单元，选中或插入（最后一个Cell的时候）下个单元\n",
    "- Ctrl-Enter : 运行本单元\n",
    "- Alt-Enter : 运行本单元，在其下插入新单元\n",
    "- Y : 单元转入代码状态\n",
    "- M :单元转入markdown状态 （目前尚不支持R 原生状态）\n",
    "- Up : 选中上方单元\n",
    "- K : 选中上方单元\n",
    "- Down : 选中下方单元\n",
    "- J : 选中下方单元\n",
    "- A : 在上方插入新单元\n",
    "- B : 在下方插入新单元\n",
    "- D,D : 删除选中的单元\n",
    "- L : 转换行号\n",
    "- Shift-Space : 向上滚动\n",
    "- Space : 向下滚动\n",
    "#### Cell编辑模式下支持的Vscode快捷键（只描述与编辑相关的那些快捷键）\n",
    "- Ctrl + X ：剪切/剪切行（空选定）\n",
    "- Ctrl + C : 复制/复制行（空选定）\n",
    "- Ctrl + Delete / Backspace :删除右边、左边的字\n",
    "- Alt + ↑ / ↓ :向上/向下移动行\n",
    "- Shift + Alt + ↓ / ↑ : 向上/向下复制行\n",
    "- Ctrl + Shift + K : 删除行\n",
    "- Ctrl + Shift + \\ : 跳到匹配的括号\n",
    "- Ctrl + ] / [ : 缩进/突出行\n",
    "- Ctrl + ← / → : 光标到字首/字尾\n",
    "- Ctrl + / : 切换行注释\n",
    "- Shift + Alt + A : 切换块注释\n",
    "- Ctrl + H : 查找/替换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss\n",
    "官方公式为：\n",
    "$$\n",
    "\\ell(x,y) = L = \\{ l_1, ..., l_N\\}^T \\\\\n",
    "l_n=-w_{y_n}\\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^{C} \\exp({x_{n,c})}}\n",
    "$$\n",
    "在下述例子中，默认使用mean，$w_{y_n}$就是1/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### base test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8053160905838013\n",
      "Loss: 1.5053160190582275\n",
      "Loss: 1.105316162109375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设模型输出是对应三个类别的概率\n",
    "# model_output.shape = [n, c] 预测一个样本，输出三个类别\n",
    "model_output = torch.tensor([[0.8, 0.1, 0.5]])\n",
    "\n",
    "# 真实标签是第一类别（索引为0-2）\n",
    "for i in range(3):\n",
    "    true_label_index = torch.tensor([i])\n",
    "\n",
    "    # 使用CrossEntropyLoss计算损失\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(model_output, true_label_index)\n",
    "\n",
    "    print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{n,y_n}$那就是$y_n$的第个$x$，这里$y_n$的$n$我倾向于batch，$x_{n,c}$就为0.3, 0.1, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1:0.8053160526833752\n",
      "l2:1.5053160526833753\n",
      "l3:1.1053160526833754\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "l1 = -math.log(math.exp(0.8)/(math.exp(0.8)+math.exp(0.1)+math.exp(0.5)))\n",
    "l2 = -math.log(math.exp(0.1)/(math.exp(0.8)+math.exp(0.1)+math.exp(0.5)))\n",
    "l3 = -math.log(math.exp(0.5)/(math.exp(0.8)+math.exp(0.1)+math.exp(0.5)))\n",
    "print(\"l1:{}\\nl2:{}\\nl3:{}\".format(l1,l2,l3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以代码更具体的应该为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l_n&=-w_{y_n}\\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^{C} \\exp(x_{n,c})}\\\\\n",
    "&=-w_{y_n}*[x_{n,y_n}+\\log(softmax(x_{n,c}))]\\\\\n",
    "&=w_{y_n}*[nn.NLLLloss()-nn.LogSoftmax()]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test with linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 1.116616862160819\n",
      "Epoch 200/1000, Loss: 0.44855516297476633\n",
      "Epoch 300/1000, Loss: 0.1374076179095677\n",
      "Epoch 400/1000, Loss: 0.11943644285202026\n",
      "Epoch 500/1000, Loss: 0.11935197455542428\n",
      "Epoch 600/1000, Loss: 0.1193519915853228\n",
      "Epoch 700/1000, Loss: 0.11935187237603324\n",
      "Epoch 800/1000, Loss: 0.11935187237603324\n",
      "Epoch 900/1000, Loss: 0.11935187237603324\n",
      "Epoch 1000/1000, Loss: 0.11935187237603324\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络结构\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 定义模型超参数\n",
    "input_size = 1  # 输入特征的大小，针对数据集格式\n",
    "hidden_size = 4  # 隐藏层大小\n",
    "output_size = 1  # 输出类别的数量\n",
    "learning_rate = 0.01\n",
    "batch_size = 1\n",
    "epochs = 1000\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleNet(input_size, hidden_size, output_size)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 准备数据\n",
    "x = np.array([1, 2, 3, 4, 1.1, 2.2, 3.3])\n",
    "y_true = x**2\n",
    "inputs = torch.FloatTensor(x).view(-1, 1)\n",
    "labels = torch.FloatTensor(y_true).view(-1, 1)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad() # 清零梯度\n",
    "    outputs = model(inputs) # 前向传播\n",
    "    loss = criterion(outputs, labels) # 计算损失\n",
    "    loss.backward() # 反向传播\n",
    "    optimizer.step() # 更新权重\n",
    "    total_loss += loss.item()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4887, -1.0781,  0.7716,  1.7271]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.T  # shape(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.4106, -1.0018,  0.1997,  0.8675]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2.weight  # shape(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 3.4327, -0.2063, -0.0263,  0.0605], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1655], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.bias)  # shape(4)\n",
    "print(model.fc2.bias)  # shape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.5754]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model inference\n",
    "model.eval()  # train mode is equal to eval in this code\n",
    "model(torch.FloatTensor([[4],]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.5754, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for evaluating the math argorithm\n",
    "input_data = 4\n",
    "out = 0\n",
    "hidden_unit = []\n",
    "for i in range(4):\n",
    "    hidden_unit.append(input_data*model.fc1.weight[i][0]+model.fc1.bias[i])\n",
    "    out += hidden_unit[i]*model.fc2.weight[0][i]\n",
    "out = out + float(model.fc2.bias)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过上述代码可以看到，模型的线性连接层确实就是加权求和  \n",
    "训练中，大概400个epoch就拟合了  \n",
    "为了证明非线性activation的有效性，下面用一层网络替代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/5000, Loss: 0.34475255012512207\n",
      "Epoch 2000/5000, Loss: 0.12732360192707606\n",
      "Epoch 3000/5000, Loss: 0.1193715078490121\n",
      "Epoch 4000/5000, Loss: 0.1193520086152213\n",
      "Epoch 5000/5000, Loss: 0.11935194901057652\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "input_size = 1 \n",
    "hidden_size = 1  # 改成一层\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 1\n",
    "epochs = 5000  # 收敛速度慢，增大训练次数\n",
    "\n",
    "model = SimpleNet(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 1.1, 2.2, 3.3])\n",
    "y_true = x**2\n",
    "inputs = torch.FloatTensor(x).view(-1, 1)\n",
    "labels = torch.FloatTensor(y_true).view(-1, 1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(x)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
